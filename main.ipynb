{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "p:0.8866 r:0.8847 f1:0.8841\n",
      "lr\n",
      "p:0.7364 r:0.7325 f1:0.7337\n",
      "knn\n",
      "p:0.8419 r:0.8402 f1:0.8381\n",
      "nb\n",
      "p:0.8467 r:0.8437 f1:0.8438\n",
      "rfc\n",
      "p:0.8525 r:0.8516 f1:0.8499\n",
      "bgg\n",
      "p:0.7675 r:0.7599 f1:0.7571\n",
      "mlp\n",
      "p:0.7015 r:0.6976 f1:0.6966\n",
      "hard voting\n",
      "p:0.8759 r:0.8758 f1:0.8748\n",
      "soft voting\n",
      "p:0.8741 r:0.8740 f1:0.8732\n",
      "stacking\n",
      "p:0.8970 r:0.8962 f1:0.8957\n"
     ]
    }
   ],
   "source": [
    "# coding: UTF-8\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.naive_bayes import GaussianNB as GNB\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from stacking import StackingClassifier\n",
    "\n",
    "def main():\n",
    "    digits = load_digits()\n",
    "    noised_data = digits.data + np.random.random(digits.data.shape)*15\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        noised_data, digits.target, test_size=0.8)\n",
    "\n",
    "    svm =SVC(C=5, gamma=0.001, probability=True)\n",
    "    lr = LogisticRegression()\n",
    "    knn = KNN(n_jobs=-1)\n",
    "    nb = GNB()\n",
    "    rfc = RFC(n_estimators=500, n_jobs=-1)\n",
    "    bgg = BaggingClassifier(n_estimators=300, n_jobs=-1)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(40, 20), max_iter=1000)\n",
    "    xgb = XGBClassifier(n_estimators=300, n_jobs=-1)\n",
    "\n",
    "    estimators = list(zip([\"svm\",\"lr\",\"knn\",\"nb\",\"rfc\",\"bgg\",\"mlp\",\"xgb\"],\n",
    "                          [svm,lr,knn,nb,rfc,bgg,mlp,xgb]))\n",
    "    \n",
    "    for name, clf in estimators:\n",
    "        clf.fit(X_train, y_train)\n",
    "        preds = clf.predict(X_test)\n",
    "        print(name)\n",
    "        print(\"p:{0:.4f} r:{1:.4f} f1:{2:.4f}\".format(\n",
    "            *precision_recall_fscore_support(y_test, preds, average=\"macro\")))\n",
    "\n",
    "    for v in [\"hard\", \"soft\"]:\n",
    "        vc_hard = VotingClassifier(estimators, voting=v)\n",
    "        vc_hard.fit(X_train, y_train)\n",
    "        preds = vc_hard.predict(X_test)\n",
    "        print(v, \"voting\")\n",
    "        print(\"p:{0:.4f} r:{1:.4f} f1:{2:.4f}\".format(\n",
    "            *precision_recall_fscore_support(y_test, preds, average=\"macro\")))\n",
    "\n",
    "    # ここから先だけ追加した\n",
    "    stcl = StackingClassifier(estimators, RFC(n_estimators=2000, n_jobs=-1))\n",
    "    stcl.fit(X_train, y_train)\n",
    "    preds = stcl.predict(X_test)\n",
    "    print(\"stacking\")\n",
    "    print(\"p:{0:.4f} r:{1:.4f} f1:{2:.4f}\".format(\n",
    "        *precision_recall_fscore_support(y_test, preds, average=\"macro\")))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('svm', SVC(C=5, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)),\n",
       " ('lr',\n",
       "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False)),\n",
       " ('knn',\n",
       "  KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "             weights='uniform')),\n",
       " ('nb', GaussianNB(priors=None)),\n",
       " ('rfc',\n",
       "  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=500, n_jobs=-1, oob_score=False,\n",
       "              random_state=None, verbose=0, warm_start=False)),\n",
       " ('bgg', BaggingClassifier(base_estimator=None, bootstrap=True,\n",
       "           bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
       "           n_estimators=300, n_jobs=-1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)),\n",
       " ('mlp',\n",
       "  MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(40, 20), learning_rate='constant',\n",
       "         learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "         shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "         verbose=False, warm_start=False)),\n",
       " ('xgb', XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "         colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "         max_depth=3, min_child_weight=1, missing=None, n_estimators=300,\n",
       "         n_jobs=-1, nthread=None, objective='binary:logistic',\n",
       "         random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "         seed=None, silent=True, subsample=1))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "noised_data = digits.data + np.random.random(digits.data.shape)*15\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    noised_data, digits.target, test_size=0.8)\n",
    "\n",
    "svm =SVC(C=5, gamma=0.001, probability=True)\n",
    "lr = LogisticRegression()\n",
    "knn = KNN(n_jobs=-1)\n",
    "nb = GNB()\n",
    "rfc = RFC(n_estimators=500, n_jobs=-1)\n",
    "bgg = BaggingClassifier(n_estimators=300, n_jobs=-1)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(40, 20), max_iter=1000)\n",
    "xgb = XGBClassifier(n_estimators=300, n_jobs=-1)\n",
    "\n",
    "estimators = list(zip([\"svm\",\"lr\",\"knn\",\"nb\",\"rfc\",\"bgg\",\"mlp\", \"xgb\"], \n",
    "                      [svm, lr, knn, nb, rfc, bgg, mlp, xgb]))\n",
    "estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "p:0.8602 r:0.8449 f1:0.8441\n",
      "lr\n",
      "p:0.7413 r:0.7360 f1:0.7356\n",
      "knn\n",
      "p:0.8547 r:0.8522 f1:0.8500\n",
      "nb\n",
      "p:0.8173 r:0.8119 f1:0.8117\n",
      "rfc\n",
      "p:0.8650 r:0.8633 f1:0.8617\n",
      "bgg\n",
      "p:0.8016 r:0.8002 f1:0.8002\n",
      "mlp\n",
      "p:0.7239 r:0.7162 f1:0.7162\n",
      "xgb\n",
      "p:0.7865 r:0.7819 f1:0.7813\n"
     ]
    }
   ],
   "source": [
    "for name, clf in estimators:\n",
    "        clf.fit(X_train, y_train)\n",
    "        preds = clf.predict(X_test)\n",
    "        print(name)\n",
    "        print(\"p:{0:.4f} r:{1:.4f} f1:{2:.4f}\".format(\n",
    "            *precision_recall_fscore_support(y_test, preds, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard voting\n",
      "p:0.8757 r:0.8743 f1:0.8728\n",
      "soft voting\n",
      "p:0.8688 r:0.8679 f1:0.8668\n"
     ]
    }
   ],
   "source": [
    "for v in [\"hard\", \"soft\"]:\n",
    "        vc_hard = VotingClassifier(estimators, voting=v)\n",
    "        vc_hard.fit(X_train, y_train)\n",
    "        preds = vc_hard.predict(X_test)\n",
    "        print(v, \"voting\")\n",
    "        print(\"p:{0:.4f} r:{1:.4f} f1:{2:.4f}\".format(\n",
    "            *precision_recall_fscore_support(y_test, preds, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacking\n",
      "p:0.8813 r:0.8796 f1:0.8787\n"
     ]
    }
   ],
   "source": [
    "# stacking\n",
    "stcl = StackingClassifier(estimators, RFC(n_estimators=2000, n_jobs=-1))\n",
    "stcl.fit(X_train, y_train)\n",
    "preds = stcl.predict(X_test)\n",
    "print(\"stacking\")\n",
    "print(\"p:{0:.4f} r:{1:.4f} f1:{2:.4f}\".format(\n",
    "        *precision_recall_fscore_support(y_test, preds, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
